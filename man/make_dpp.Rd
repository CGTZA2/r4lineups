% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dpp_functions.R
\name{make_dpp}
\alias{make_dpp}
\title{Compute Deviation from Perfect Performance (DPP)}
\usage{
make_dpp(data, lineup_size = 6, use_roc_obj = FALSE)
}
\arguments{
\item{data}{A dataframe with the following columns:
\itemize{
  \item target_present: Logical. TRUE if guilty suspect in lineup
  \item identification: Character. "suspect", "filler", or "reject"
  \item confidence: Numeric. Confidence rating
}}

\item{lineup_size}{Integer. Number of people in lineup (default = 6)}

\item{use_roc_obj}{Logical. If FALSE (default), computes ROC from data.
If TRUE, data should be the output from make_rocdata().}
}
\value{
A list containing:
  \itemize{
    \item dpp: Deviation from Perfect Performance (0 = perfect, 1 = worst)
    \item auc_observed: Area under observed ROC curve
    \item auc_perfect: Area under perfect ROC curve (for same FA range)
    \item roc_data: ROC curve data
    \item max_fa: Maximum false alarm rate in observed data
  }
}
\description{
Computes DPP metric following Smith et al. (2018). DPP measures how much
an observed ROC curve deviates from perfect performance, providing a
single-number summary that is less affected by ROC truncation than pAUC.
}
\details{
DPP (Deviation from Perfect Performance) compares the observed ROC curve
to a perfect ROC curve within the same false alarm rate range:

\deqn{DPP = 1 - \frac{AUC_{observed}}{AUC_{perfect}}}

Where:
\itemize{
  \item \strong{Perfect ROC}: Goes from (0,0) → (0,1) → (max_FA,1)
    (immediate jump to 100% hit rate, then horizontal)
  \item \strong{Observed ROC}: Actual performance from data
  \item \strong{AUC}: Area under curve computed via trapezoidal rule
}

**Interpretation**:
\itemize{
  \item DPP = 0: Perfect performance (all correct IDs at 0% false alarms)
  \item DPP → 1: Very poor performance (near chance)
  \item Lower DPP = better performance
}

**Advantages over pAUC** (Smith et al. 2018):
\itemize{
  \item Less affected by ROC truncation (different confidence distributions)
  \item Normalized relative to best achievable performance
  \item More consistent rankings across datasets
  \item Accounts for achievable performance given observed FA range
}

The perfect ROC is constrained to the same FA range as the observed data,
making DPP a fair measure even when procedures produce different confidence
distributions (and thus different ROC truncation points).
}
\references{
Smith, A. M., Wilford, M. M., Quigley-McBride, A., & Wells, G. L. (2019).
Mistaken eyewitness identification rates increase when either witnessing or
testing conditions get worse. \emph{Law and Human Behavior, 43}(4), 358-368.

Smith, A. M., et al. (2018). Deviation from perfect performance measures
the diagnostic utility of eyewitness lineups but partial area under the
ROC does not. \emph{Journal of Applied Research in Memory and Cognition}.
}
