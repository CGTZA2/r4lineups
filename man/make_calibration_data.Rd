% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/calibration_functions.R
\name{make_calibration_data}
\alias{make_calibration_data}
\title{Compute Calibration Statistics for Eyewitness Identification}
\usage{
make_calibration_data(
  data,
  confidence_bins = NULL,
  choosers_only = TRUE,
  lineup_size = 6
)
}
\arguments{
\item{data}{A dataframe with the following columns:
\itemize{
  \item target_present: Logical. TRUE if guilty suspect in lineup
  \item identification: Character. "suspect", "filler", or "reject"
  \item confidence: Numeric. Confidence rating
}}

\item{confidence_bins}{Numeric vector of bin edges for grouping confidence
(e.g., c(0, 60, 80, 100) creates bins 0-60, 60-80, 80-100).
If NULL, uses individual confidence levels.}

\item{choosers_only}{Logical. If TRUE (default), only analyze suspect identifications.
If FALSE, analyze all responses (including fillers and rejections).}

\item{lineup_size}{Integer. Number of people in lineup (default = 6).
Used for estimating incorrect suspect IDs from filler choices in target-absent lineups.}
}
\value{
A list containing:
  \itemize{
    \item calibration_data: Dataframe with per-bin confidence, accuracy, and counts
    \item C: Calibration statistic (weighted mean squared difference between confidence and accuracy)
    \item OU: Over/underconfidence (mean confidence minus mean accuracy)
    \item NRI: Normalized Resolution Index (standardized within-person variance)
    \item overall_accuracy: Mean accuracy across all responses
    \item overall_confidence: Mean confidence across all responses
    \item n_total: Total number of responses analyzed
  }
}
\description{
Computes calibration analysis metrics following Juslin, Olsson, & Winman (1996).
Calibration analysis assesses the match between confidence and accuracy, providing
a more appropriate measure than simple confidence-accuracy correlation.
}
\details{
Calibration analysis distinguishes between:
\itemize{
  \item \strong{Calibration (C)}: How well confidence matches accuracy (perfect = 0)
  \item \strong{Over/underconfidence (O/U)}: Overall bias in confidence judgments (0 = perfectly calibrated)
  \item \strong{Resolution (NRI)}: Ability to discriminate correct from incorrect responses
}

The calibration statistic C measures the weighted mean squared deviation between
mean confidence and accuracy in each bin:
\deqn{C = \sum_{j=1}^{J} \frac{n_j}{N} (c_j - a_j)^2}

The Normalized Resolution Index (NRI) captures how well confidence discriminates
correct from incorrect responses:
\deqn{NRI = \frac{\frac{1}{N} \sum_{j=1}^{J} n_j (a_j - \bar{a})^2}{\bar{a}(1-\bar{a})}}

When \code{choosers_only = TRUE}, only suspect identifications are included (standard
for eyewitness calibration analysis). When \code{choosers_only = FALSE}, all responses
are included with fillers and rejections counted as incorrect.
}
\references{
Juslin, P., Olsson, N., & Winman, A. (1996). Calibration and diagnosticity
of confidence in eyewitness identification: Comments on what can be inferred
from the low confidence-accuracy correlation. \emph{Journal of Experimental
Psychology: Learning, Memory, and Cognition, 22}(5), 1304-1316.

Brewer, N., & Wells, G. L. (2006). The confidence-accuracy relationship in
eyewitness identification: Effects of lineup instructions, foil similarity,
and target-absent base rates. \emph{Journal of Experimental Psychology:
Applied, 12}(1), 11-30.
}
