% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bayes_curves_functions.R, R/eig_functions.R
\name{entropy}
\alias{entropy}
\title{Compute Entropy for a Probability}
\usage{
entropy(p, base = 2)

entropy(p, base = 2)
}
\arguments{
\item{p}{Probability value (between 0 and 1)}

\item{base}{Logarithm base (default = 2 for bits)}
}
\value{
Entropy value in bits

Shannon entropy in bits (if base = 2)
}
\description{
Helper function to compute binary entropy H(p) = -[p*log2(p) + (1-p)*log2(1-p)]

Helper function to compute Shannon entropy for a probability value.
Entropy measures uncertainty about guilt/innocence.
}
\details{
Entropy measures uncertainty in a binary outcome. It ranges from 0 (complete certainty)
to 1 (maximum uncertainty at p=0.5). Returns 0 when p is 0 or 1 (no uncertainty).

Entropy is computed as: H(p) = -[p*log(p) + (1-p)*log(1-p)]
When p is 0 or 1 (certainty), entropy is 0.
Maximum entropy (1 bit) occurs at p = 0.5 (maximum uncertainty).
}
\keyword{internal}
